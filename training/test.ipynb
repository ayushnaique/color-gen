{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8490ecd2-3a12-4cb0-bccd-1b39190b137a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/avk3358/color-gen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd /scratch/avk3358/color-gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9d6f93-5b5a-4d02-8a76-4c591b3949a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/scratch/avk3358/color-gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ff908b1-01a1-4ff3-a3e6-00633b038015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging improved.\n"
     ]
    }
   ],
   "source": [
    "from share import *\n",
    "import config\n",
    "\n",
    "import cv2\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from annotator.util import resize_image, HWC3\n",
    "from annotator.canny import CannyDetector\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from cldm.ddim_hacked import DDIMSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "033b1c39-1fa1-401b-8af2-71f233106860",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_PROMPT_DEFAULT = \"best quality, extremely detailed\"\n",
    "N_PROMPT_DEFAULT = \"longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality\"\n",
    "\n",
    "def apply_color(image, color_map):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    color_map = cv2.cvtColor(color_map, cv2.COLOR_RGB2LAB)\n",
    "\n",
    "    l, _, _ = cv2.split(image)\n",
    "    _, a, b = cv2.split(color_map)\n",
    "\n",
    "    merged = cv2.merge([l, a, b])\n",
    "\n",
    "    result = cv2.cvtColor(merged, cv2.COLOR_LAB2RGB)\n",
    "    return result\n",
    "\n",
    "def run_sampler(\n",
    "    model,\n",
    "    input_image: np.ndarray,\n",
    "    prompt: str,\n",
    "    num_samples: int = 1,\n",
    "    image_resolution: int = 256,\n",
    "    seed: int = -1,\n",
    "    a_prompt: str = A_PROMPT_DEFAULT,\n",
    "    n_prompt: str = N_PROMPT_DEFAULT,\n",
    "    guess_mode=False,\n",
    "    strength=1.0,\n",
    "    ddim_steps=20,\n",
    "    eta=0.0,\n",
    "    scale=9.0,\n",
    "    show_progress: bool = True,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "\n",
    "        ddim_sampler = DDIMSampler(model)\n",
    "\n",
    "        img = resize_image(HWC3(input_image), image_resolution)\n",
    "        H, W, C = img.shape\n",
    "\n",
    "        # detected_map = np.zeros_like(img, dtype=np.uint8)\n",
    "        # detected_map[np.min(img, axis=2) < 127] = 255\n",
    "\n",
    "        control = torch.from_numpy(img.copy()).float().cuda() / 255.0\n",
    "        control = torch.stack([control for _ in range(num_samples)], dim=0)\n",
    "        control = einops.rearrange(control, \"b h w c -> b c h w\").clone()\n",
    "\n",
    "        if seed == -1:\n",
    "            seed = random.randint(0, 65535)\n",
    "        seed_everything(seed)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        cond = {\n",
    "            \"c_concat\": [control],\n",
    "            \"c_crossattn\": [\n",
    "                model.get_learned_conditioning([prompt + \", \" + a_prompt] * num_samples)\n",
    "            ],\n",
    "        }\n",
    "        un_cond = {\n",
    "            \"c_concat\": None if guess_mode else [control],\n",
    "            \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)],\n",
    "        }\n",
    "        shape = (4, H // 8, W // 8)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=True)\n",
    "\n",
    "        model.control_scales = (\n",
    "            [strength * (0.825 ** float(12 - i)) for i in range(13)]\n",
    "            if guess_mode\n",
    "            else ([strength] * 13)\n",
    "        )  # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n",
    "        samples, intermediates = ddim_sampler.sample(\n",
    "            ddim_steps,\n",
    "            num_samples,\n",
    "            shape,\n",
    "            cond,\n",
    "            verbose=False,\n",
    "            eta=eta,\n",
    "            unconditional_guidance_scale=scale,\n",
    "            unconditional_conditioning=un_cond,\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        x_samples = model.decode_first_stage(samples)\n",
    "        x_samples = (\n",
    "            (einops.rearrange(x_samples, \"b c h w -> b h w c\") * 127.5 + 127.5)\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "            .clip(0, 255)\n",
    "            .astype(np.uint8)\n",
    "        )\n",
    "\n",
    "        results = [x_samples[i] for i in range(num_samples)]\n",
    "        colored_results = [apply_color(img, result) for result in results]\n",
    "\n",
    "        return [img] + results + colored_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0199f81d-4757-49e0-8452-5f9986117c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Loaded model config from [./models/cldm_v15.yaml]\n",
      "Loaded state_dict from [./models/prompt.ckpt]\n"
     ]
    }
   ],
   "source": [
    "apply_canny = CannyDetector()\n",
    "checkpoint_path = './models/prompt.ckpt'\n",
    "model = create_model('./models/cldm_v15.yaml').cpu()\n",
    "model.load_state_dict(load_state_dict(checkpoint_path, location='cuda'))\n",
    "model = model.cuda()\n",
    "ddim_sampler = DDIMSampler(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e997be6f-1da7-4fde-a65b-31bde328ef1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image = cv2.imread('/scratch/work/public/imagenet/train/n07590611/n07590611_9304.JPEG')\n",
    "image = cv2.imread('./training/goldfish.png')\n",
    "cv2.imwrite('./training/input.JPEG', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b89b236-714f-4e32-88ea-eb063eed0cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateResults(image, size=256, prompt=\"\"):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB if necessary\n",
    "    image = image.astype(np.uint8)  # Normalize the image to [0, 1]\n",
    "    \n",
    "    results = run_sampler(\n",
    "        model = model,\n",
    "        input_image = image,\n",
    "        prompt = prompt,\n",
    "        image_resolution = size\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06761cc8-c5e8-4d80-a2d9-c3a494bf16a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 9939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 88), eta 0.0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:09<00:00,  2.11it/s]\n"
     ]
    }
   ],
   "source": [
    "results = generateResults(image, size=512, prompt=\"color the fish in pink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30c8075b-da10-4fab-9244-ae304fb5fc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('./training/output.JPEG', results[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
